{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Document Similarity: FDR's Inaugural Addresses</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jared Neumann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "For this project, I will compare the four inaugural addresses of Franklin Delano Roosevelt given in 1933, 1937, 1941, and 1945. I will rely on just a few packages, such as nltk and math, while using pandas for display purposes, and scipy only for the last part on hierarchical clustering.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Import Statements</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, math\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    First, we need to retrieve the documents that will be used for the project, namely, the four inaugural addresses, from the nltk corpora. This order corresponds to the indices [0-3] respectively in the results.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_1933 = nltk.corpus.inaugural.raw('1933-Roosevelt.txt')\n",
    "fdr_1937 = nltk.corpus.inaugural.raw('1937-Roosevelt.txt')\n",
    "fdr_1941 = nltk.corpus.inaugural.raw('1941-Roosevelt.txt')\n",
    "fdr_1945 = nltk.corpus.inaugural.raw('1945-Roosevelt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_corpus_raw = [fdr_1933, fdr_1937, fdr_1941, fdr_1945]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to clean up the texts. They will each be tokenized using the RegexpTokenizer in order to retain only alphanumeric characters (ignoring punctuation). All tokens will be made lowercase, stopwords (provided by nltk) will be removed, and words will be lemmatized. The results are added to a new list of the processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_corpus_processed = []\n",
    "\n",
    "#Use the RegexpTokenizer to retrieve alphumeric strings.\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#Use the built-in stopwords list; this is not ideal, esp. considering the age of the texts.\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#Use the built-in WordNetLemmatizer; there is some difficulty here due to spelling changes over the years.\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "#Put the aforementioned to use step-by step in each doc.\n",
    "for doc in fdr_corpus_raw:\n",
    "    doc_tokenized = tokenizer.tokenize(doc)\n",
    "    doc_lowercase = [x.lower() for x in doc_tokenized] #List comprehension\n",
    "    doc_nostopwords = [x for x in doc_lowercase if not x in stop_words]\n",
    "    doc_lemmatized = [lemmatizer.lemmatize(x) for x in doc_nostopwords]\n",
    "    \n",
    "    #Append the result to the new list.\n",
    "    fdr_corpus_processed.append(doc_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function Definitions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a few functions that will be helpful later. Since we will be finding tf-idf values for the corpus, we will need a wordlist for the whole corpus and counts for the words in their respective documents. The approach taken here and for the other tf-idf functions was partially taken from <a href=https://github.com/mayank408/TFIDF>mayank408</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcounts(corpus):\n",
    "    #Create a list for sets of words corresponding to each individual doc.\n",
    "    docs_bow = []\n",
    "    \n",
    "    #Generate those sets and add them to the list.\n",
    "    for doc in corpus:\n",
    "        docs_bow.append(set(doc))\n",
    "\n",
    "    #Create a general set of words for the whole corpus.\n",
    "    corpus_bow = set.union(*docs_bow)\n",
    "\n",
    "    #Create a list of dictionaries for wordcounts associated with each doc.\n",
    "    word_dicts = [dict.fromkeys(corpus_bow, 0) for i in range(0, len(docs_bow))]\n",
    "\n",
    "    #Count the number of occurrences of each word in the complete wordlist\n",
    "    #for each document.\n",
    "    for doc in corpus:\n",
    "        for word in doc:\n",
    "            word_dicts[corpus.index(doc)][word] += 1\n",
    "\n",
    "    return word_dicts, docs_bow, corpus_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three definitions are for measuring tf-idf values for the words in the corpus. Term frequency (tf) is defined as the number of times a term (t) occurs in a document (d) divided by the total number of words (t', where t' &isin; d):\n",
    "f<sub>t,d</sub> / &sum;f<sub>t',d</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word_dict, doc_bow):\n",
    "\n",
    "    tf_dict = {}\n",
    "    \n",
    "    #this function only applies to one doc at a time, so we can just grab the wordcount from the doc that has been passed.\n",
    "    wordcount = len(doc_bow)\n",
    "\n",
    "    #update the dictionary with the tf math.\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count/float(wordcount)\n",
    "\n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Inverse Document Frequency (idf) is defined as the logarithm of the total number of documents (N) divided by the number of documents that contain a given term (df<sub>t</sub>): log(N / df<sub>t</sub>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word_dicts):\n",
    "    \n",
    "    #word_dicts is our list of wordcounts in each doc.\n",
    "    N = len(word_dicts)\n",
    "\n",
    "    #initialize a dictionary with one of the wordlists in word_dicts.\n",
    "    idf_dict = dict.fromkeys(word_dicts[0].keys(), 0)\n",
    "\n",
    "    #update the count of the word if it appears in a doc.\n",
    "    #only count it once per doc.\n",
    "    for word_dict in word_dicts:\n",
    "        for word, count in word_dict.items():\n",
    "            if count > 0:\n",
    "                idf_dict[word] += 1\n",
    "\n",
    "    #update the count with the idf math\n",
    "    for word, count in idf_dict.items():\n",
    "        idf_dict[word] = math.log10(N/float(count))\n",
    "\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, put the pieces together. Although, it should be noted that the idf scores must be calculated before the following function is useful, since they are one of the arguments. This function simply multiplies the two values together for each word. Again, this is meant to be used in a <i>for-loop</i> along with tf()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(tf_dict, idf_scores):\n",
    "\n",
    "    tfidf = {}\n",
    "\n",
    "    #Update the dictionary with the values from our tf function multiplied by the scores from our idf function.\n",
    "    for word, count in tf_dict.items():\n",
    "        tfidf[word] = count * idf_scores[word]\n",
    "\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two remaining functions are to calculate different similarity metrics: cosine similarity and jaccard similarity. The former makes use of our tfidf scores, whereas the latter just needs the tokenized documents. Cosine similarity calculates the similarity between two vectors (e.g., the ones with the tfidf values corresponding to the words in each document) by taking the dot product of the two vectors and dividing it by their lengths multiplied together. Distance is obtained by subtracting the result from 1, which will be done <i>in situ</i> at execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v_1, v_2):\n",
    "\n",
    "    #definition of dot product\n",
    "    dot_product = sum(a * b for a,b in zip(v_1, v_2))\n",
    "\n",
    "    #definition of norm (length) of a vector\n",
    "    norm_v1 = sum(a * a for a in v_1) ** 0.5\n",
    "    norm_v2 = sum(b * b for b in v_2) ** 0.5\n",
    "\n",
    "    #putting it together\n",
    "    return dot_product/float(norm_v1 * norm_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similarity, on the other hand, calculates the similarity between two sets by taking their intersection and dividing it by their union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacc_sim(v_1, v_2):\n",
    "\n",
    "    #all the math takes place here, since we have already converted the documents to sets.\n",
    "    return len(v_1.intersection(v_2))/float(len(v_1.union(v_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Execution</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, grab the variables that will be needed by running wordcounts() and idf()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dicts, docs_bow, corpus_bow = wordcounts(fdr_corpus_processed)\n",
    "idf_scores = idf(word_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, generate the tf-idf scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_scores = []\n",
    "for word_dict in word_dicts:\n",
    "    i = word_dicts.index(word_dict)\n",
    "    tfidf_scores.append(tfidf(tf(word_dict, docs_bow[i]), idf_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Tf-Idf Values Table:</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   make     every  multiplied  suffering  individual   falsity  purpose  \\\n",
      "0   0.0  0.000840    0.001012   0.000000    0.000000  0.001012      0.0   \n",
      "1   0.0  0.000875    0.000000   0.002109    0.001582  0.000000      0.0   \n",
      "2   0.0  0.000921    0.000000   0.000000    0.000740  0.000000      0.0   \n",
      "3   0.0  0.000000    0.000000   0.000000    0.000000  0.000000      0.0   \n",
      "\n",
      "   disruption  assurance   carried  ...  mistrust      draw      ever  \\\n",
      "0    0.000000   0.002024  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "1    0.000000   0.000000  0.000000  ...  0.000000  0.001054  0.003163   \n",
      "2    0.002959   0.000000  0.001479  ...  0.000000  0.000000  0.000000   \n",
      "3    0.000000   0.000000  0.000000  ...  0.003056  0.000000  0.000000   \n",
      "\n",
      "    primary     order  time  faithful    temple  permanently     light  \n",
      "0  0.001012  0.001012   0.0  0.000506  0.001012     0.001012  0.000000  \n",
      "1  0.000000  0.001582   0.0  0.000527  0.000527     0.000000  0.001054  \n",
      "2  0.000000  0.000000   0.0  0.000000  0.000000     0.000000  0.000000  \n",
      "3  0.000000  0.000000   0.0  0.000000  0.000000     0.000000  0.000000  \n",
      "\n",
      "[4 rows x 1273 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(tfidf_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, extract the vectors needs to calculate similarities between each individual document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = []\n",
    "for score in tfidf_scores:\n",
    "    tfidf_vectors.append(list(score.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, calculate cosine similarity between all vectors and construct a matrix out of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_matrix = []\n",
    "for i in tfidf_vectors:\n",
    "    cos_sim_row = []\n",
    "    for j in tfidf_vectors:\n",
    "        cos_sim_row.append(cos_sim(i,j))\n",
    "    cos_sim_matrix.append(cos_sim_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Cosine Similarity Matrix</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  1.000000  0.097259  0.042982  0.022202\n",
      "1  0.097259  1.000000  0.064447  0.054338\n",
      "2  0.042982  0.064447  1.000000  0.028709\n",
      "3  0.022202  0.054338  0.028709  1.000000\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(cos_sim_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth, calculate jaccard similarity between all vectors and construct another matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacc_sim_matrix = []\n",
    "for i in docs_bow:\n",
    "    jacc_sim_row = []\n",
    "    for j in docs_bow:\n",
    "        jacc_sim_row.append(jacc_sim(i,j))\n",
    "    jacc_sim_matrix.append(jacc_sim_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Jaccard Similarity Matrix</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  1.000000  0.189796  0.147766  0.106145\n",
      "1  0.189796  1.000000  0.182588  0.126100\n",
      "2  0.147766  0.182588  1.000000  0.146110\n",
      "3  0.106145  0.126100  0.146110  1.000000\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(jacc_sim_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Clustering</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to conduct hierearchical clustering, it is necessary to convert the cosine similarity matrix obtained above into a distance matrix. Recall that it is obtained by subtracting the similarity scores from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_dist_matrix = []\n",
    "for i in tfidf_vectors:\n",
    "    cos_dist_row = []\n",
    "    for j in tfidf_vectors:\n",
    "        cos_dist_row.append(1-cos_sim(i,j))\n",
    "    cos_dist_matrix.append(cos_dist_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relying on SciPy, use ward linkage to conduct the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage = sch.linkage(cos_dist_matrix, method='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Hierarchical Clustering Table</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.         1.2772527  2.        ]\n",
      " [2.         4.         1.3592457  3.        ]\n",
      " [3.         5.         1.38880722 4.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(linkage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Dendrogram</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, draw a dendrogram that shows the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dendrogram = sch.dendrogram(linkage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I interpret the dendrogram, it would seem as though the first two of FDR's inaugural addresses are clustered together, and the last two do not have the same cohesion. It is interesting to not that the last address (3) is very short, so that may have made more appropriate clustering according to the given metrics difficult. There are some other limitations given the approach I have taken here. First, I decided to lemmatize rather than stem, but that choice was largely arbitrary. I do believe that tense might be an interesting rhetorical aspect that is overlooked by lemmatization. However, both the WordNetLemmatizer and the PorterStemmer could not manage the voocabulary in the texts, and I preferred the Lemmatizer on this qualitative measure. It would be interesting to compare the two overall, though. Finally, I hoped to uncover some trend in the addresses, but find it difficult to draw any firm conclusions. One that I think is supported by the evidence, at least, is that the content of the speeches seems to have changed successively. Might this mean that there is not a close correspondence between <i>first</i> inaugural addresses? Or that there is some kind of cohesion between terms in an administration? These are questions for a broader study."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
